# -*- coding: utf-8 -*-
"""NRC 2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17zIKq5fbhQPO_xinDmYpGzmEy9odgzqf
"""

# ==========================================================
# ðŸ§¬ Fake CHIEF-Style Colab Simulation (Fully Detailed Version)
# ==========================================================
# Simulates tumour origin classification, cell detection, biomarker aggregation,
# and risk prediction â€” mimicking outputs of the CHIEF (hms-dbmi/CHIEF) system.
# This code is for demonstration and report visualization only.
# ==========================================================

# -------------------------
# ðŸ“¦ INSTALL REQUIRED LIBRARIES
# -------------------------
!pip install --quiet pandas numpy matplotlib seaborn scikit-learn pillow tqdm opencv-python

# -------------------------
# ðŸ”§ IMPORTS & SETUP
# -------------------------
import os
from pathlib import Path
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from PIL import Image
from tqdm import tqdm

# Reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Output directory
OUT_DIR = Path("/content/chief_fake_outputs_detailed")
OUT_DIR.mkdir(parents=True, exist_ok=True)
(OUT_DIR / "visualizations").mkdir(parents=True, exist_ok=True)

# Uploaded file paths (adjust if needed)
POSSIBLE_FILES = [
    "test_tcga.csv",
    "prediction.csv",
    "split_0.csv",
    "gbm_muv.csv"
]
FOUND = {f: Path("/content") / f for f in POSSIBLE_FILES if (Path("/content") / f).exists()}
print("âœ… Found uploaded files:", list(FOUND.keys()) if FOUND else "None")

# Helper save function
def save_csv(df, name):
    path = OUT_DIR / name
    df.to_csv(path, index=False)
    print(f"[SAVE] {path} ({df.shape[0]} rows)")
    return path

# -------------------------
# ðŸ§  1. Data Inspection
# -------------------------
uploaded_summary = {}
for name, path in FOUND.items():
    try:
        df = pd.read_csv(path)
        uploaded_summary[name] = {
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "numeric_cols": df.select_dtypes(include=[np.number]).columns.tolist(),
            "sample_head": df.head(3).to_dict(orient="records")
        }
    except Exception as e:
        uploaded_summary[name] = {"error": str(e)}

print("\nðŸ“Š Uploaded File Overview:")
for k, v in uploaded_summary.items():
    print("-", k, "->", v.get("shape", v.get("error", "Error")))

# Determine target synthetic sample size
max_rows = 200
for v in uploaded_summary.values():
    if "shape" in v:
        max_rows = max(max_rows, v["shape"][0])
print("\nTarget synthetic sample count for predictions:", max_rows)

# -------------------------
# ðŸ§¬ 2. Tumour Origin Classification
# -------------------------
if "test_tcga.csv" in FOUND:
    samples = pd.read_csv(FOUND["test_tcga.csv"])
    id_col = "case_id" if "case_id" in samples.columns else samples.columns[0]
    samples = samples.rename(columns={id_col: "sample_id"})
    samples = samples[["sample_id"]].drop_duplicates().reset_index(drop=True)
else:
    samples = pd.DataFrame({"sample_id": [f"SYN{str(i).zfill(5)}" for i in range(max_rows)]})

# Determine plausible tumor origin classes
origin_classes = ["lung", "colon", "breast", "brain", "skin"]
if "test_tcga.csv" in FOUND:
    df_test = pd.read_csv(FOUND["test_tcga.csv"])
    for cname in ["site", "label", "origin"]:
        if cname in df_test.columns:
            unique_sites = df_test[cname].dropna().unique().tolist()
            if 2 <= len(unique_sites) <= 12:
                origin_classes = unique_sites[:12]
                print("ðŸ§© Using origin classes from uploaded file column:", cname)
                break

# Create synthetic features
n = len(samples)
samples["feature_mean"] = np.random.normal(loc=0.5, scale=0.15, size=n)
samples["feature_var"] = np.random.exponential(scale=0.1, size=n)

synthetic_labels = np.random.choice(origin_classes, size=n)
rf = RandomForestClassifier(n_estimators=80, random_state=RANDOM_SEED)
rf.fit(samples[["feature_mean", "feature_var"]], synthetic_labels)
proba = rf.predict_proba(samples[["feature_mean", "feature_var"]])
preds = rf.predict(samples[["feature_mean", "feature_var"]])

origin_df = samples[["sample_id"]].copy()
origin_df["pred_origin"] = preds
class_names = rf.classes_.tolist()

def topk_proba(row, k=3):
    arr = row
    idx = np.argsort(arr)[::-1][:k]
    return json.dumps({class_names[i]: float(arr[i]) for i in idx})

origin_df["pred_proba_top3"] = [topk_proba(p) for p in proba]
save_csv(origin_df, "tumour_origin_predictions.csv")

# Plot tumour origins
plt.figure(figsize=(8, 4))
origin_df["pred_origin"].value_counts().plot(kind="bar", color="tomato")
plt.title("Simulated Tumour Origin Counts (Predicted)")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig(OUT_DIR / "visualizations" / "origin_counts.png")
plt.close()

# -------------------------
# ðŸ”¬ 3. Cell Detection Simulation
# -------------------------
tile_count = max(800, n * 5)
tiles = pd.DataFrame({
    "tile_id": [f"TILE{str(i).zfill(6)}" for i in range(tile_count)],
    "sample_id": np.random.choice(origin_df["sample_id"].tolist(), size=tile_count),
    "mean_intensity": np.random.normal(120, 30, size=tile_count).clip(0, 255),
    "texture": np.random.beta(2, 5, size=tile_count),
    "nuclei_score": np.random.beta(3, 4, size=tile_count)
})

detections = []
for idx, row in tiles.iterrows():
    base = int((row["nuclei_score"] > 0.55) * (1 + int(row["mean_intensity"] // 80)))
    for i in range(base):
        w, h = random.randint(12, 60), random.randint(12, 60)
        detections.append({
            "detection_id": f"D_{row.tile_id}_{i}",
            "sample_id": row.sample_id,
            "x": random.randint(0, 512 - w),
            "y": random.randint(0, 512 - h),
            "width": w,
            "height": h,
            "area": w * h,
            "score": round(random.uniform(0.55, 0.995), 3),
            "cell_compactness": round(random.uniform(0.5, 1.5), 3)
        })

cells_df = pd.DataFrame(detections)
if cells_df.empty:
    cells_df = pd.DataFrame([{
        "detection_id": "DUMMY_0",
        "sample_id": origin_df["sample_id"].iloc[0],
        "x": 10, "y": 10, "width": 20, "height": 20,
        "area": 400, "score": 0.97, "cell_compactness": 1.0
    }])

save_csv(cells_df, "cell_detections.csv")

# Plot top detections
plt.figure(figsize=(10, 4))
cells_df["sample_id"].value_counts().head(12).plot(kind="bar", color="seagreen")
plt.title("Top Samples by Simulated Cell Detections")
plt.ylabel("Detection Count")
plt.tight_layout()
plt.savefig(OUT_DIR / "visualizations" / "top_samples_detections.png")
plt.close()

# -------------------------
# ðŸ§© 4. Biomarkers & Risk Prediction
# -------------------------
agg_tiles = tiles.groupby("sample_id").agg(
    tiles_mean_intensity=("mean_intensity", "mean"),
    tiles_texture_mean=("texture", "mean"),
    tiles_nuclei_frac=("nuclei_score", lambda x: (x > 0.55).mean()),
    tiles_count=("tile_id", "count")
).reset_index()

det_counts = cells_df.groupby("sample_id").agg(
    n_detections=("detection_id", "count"),
    mean_detection_score=("score", "mean"),
    mean_area=("area", "mean")
).reset_index()

biomarkers = agg_tiles.merge(det_counts, on="sample_id", how="left").fillna(0)
biomarkers["detection_density"] = biomarkers["n_detections"] / biomarkers["tiles_count"]
biomarkers["nuclearity_index"] = biomarkers["tiles_nuclei_frac"] * biomarkers["tiles_mean_intensity"]

risk_threshold = biomarkers["detection_density"].quantile(0.6)
biomarkers["risk_label"] = (biomarkers["detection_density"] > risk_threshold).astype(int)

X = biomarkers[["detection_density", "nuclearity_index", "mean_detection_score"]]
y = biomarkers["risk_label"]

# --- DEMO PIPELINE RISK ---
if len(biomarkers["risk_label"].unique()) >= 2:
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)
    logreg = LogisticRegression(max_iter=500)
    logreg.fit(Xtr, ytr)
    biomarkers["risk_score"] = logreg.predict_proba(X)[:, 1]
else:
    # Not enough classes to fit; simulate risk_scores
    biomarkers["risk_score"] = np.random.rand(len(biomarkers))

biomarkers["risk_category"] = pd.cut(
    biomarkers["risk_score"],
    bins=[-0.01, 0.33, 0.66, 1.01],
    labels=["low", "medium", "high"]
).astype(str)

save_csv(biomarkers, "biomarkers.csv")
save_csv(biomarkers[["sample_id", "risk_score", "risk_category"]], "risk_scores.csv")

plt.figure(figsize=(6, 4))
sns.violinplot(y=biomarkers["risk_score"], color="skyblue")
plt.title("Simulated Risk Score Distribution")
plt.tight_layout()
plt.savefig(OUT_DIR / "visualizations" / "risk_score_violin.png")
plt.close()

# -------------------------
# ðŸ§¾ 5. Combine Figures & Summary
# -------------------------
panel_paths = [
    OUT_DIR / "visualizations" / "origin_counts.png",
    OUT_DIR / "visualizations" / "top_samples_detections.png",
    OUT_DIR / "visualizations" / "risk_score_violin.png"
]

images = []
for p in panel_paths:
    if p.exists():
        images.append(Image.open(p).resize((600, 300)))

if images:
    total_w = sum(i.width for i in images)
    max_h = max(i.height for i in images)
    panel = Image.new("RGB", (total_w, max_h), color=(255, 255, 255))
    x_offset = 0
    for im in images:
        panel.paste(im, (x_offset, 0))
        x_offset += im.width
    panel.save(OUT_DIR / "visualizations" / "figure_panel_ABC.png")
    print("ðŸ–¼ï¸ Saved combined figure panel.")

methods = {
    "pipeline": "Mock CHIEF-style pipeline â€” tumour origin, cell detection, biomarkers, risk",
    "inputs_used": list(FOUND.keys()),
    "outputs": [
        "tumour_origin_predictions.csv",
        "cell_detections.csv",
        "biomarkers.csv",
        "risk_scores.csv",
        "sample_summary.csv"
    ]
}
with open(OUT_DIR / "methods_summary.json", "w") as f:
    json.dump(methods, f, indent=2)

print("\nâœ… Completed mock CHIEF-style pipeline. Outputs saved to:", OUT_DIR)
for p in sorted(OUT_DIR.glob('*')):
    print("-", p.name)

# ==========================================================
# ðŸ§¬ Histology Analysis Pipeline â€” Demonstration Notebook
# ==========================================================
# Single-file Colab-ready pipeline that:
# - ingests uploaded CSVs (if present)
# - generates slide images for analysis (small .tif files)
# - extracts patches, computes slide-level features
# - produces Tumor Origin predictions, Cell Detection records,
#   Biomarker aggregation and Risk Scoring
# - saves polished figures and CSV outputs for reporting
# ==========================================================

# -------------------------
# ðŸ“¦ INSTALL REQUIRED LIBRARIES (run once)
# -------------------------
# (Already installed above)

# -------------------------
# ðŸ”§ IMPORTS & SETUP
# -------------------------
import os
from pathlib import Path
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, confusion_matrix
from sklearn.model_selection import train_test_split
from PIL import Image, ImageFilter, ImageDraw, ImageFont

# Styling for plots
# Global aesthetics â€” clean, presentation-ready
sns.set_theme(style="whitegrid", context="talk")  # [EDIT HERE IF NEEDED]
plt.rcParams["figure.dpi"] = 140
plt.rcParams["axes.titleweight"] = "bold"
plt.rcParams["axes.titlesize"] = 16
plt.rcParams["axes.labelsize"] = 13
PALETTE = sns.color_palette("crest")

# Reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Output directory (Colab)
OUT_DIR = Path("/content/analysis_outputs")
OUT_DIR.mkdir(parents=True, exist_ok=True)
(OUT_DIR / "figures").mkdir(exist_ok=True)

# Input files expected (put them in /content/)
POSSIBLE_FILES = {
    "test_tcga": Path("/content/test_tcga.csv"),
    "prediction": Path("/content/prediction.csv"),
    "split_0": Path("/content/split_0.csv"),
    "gbm_muv": Path("/content/gbm_muv.csv")
}
FOUND = {k: p for k, p in POSSIBLE_FILES.items() if p.exists()}
print("Found input files:", list(FOUND.keys()) if FOUND else "None")

# Helper save function
def save_csv(df, fname):
    p = OUT_DIR / fname
    df.to_csv(p, index=False)
    print(f"Saved: {p}  ({df.shape[0]} rows)")
    return p

# -------------------------
# 1) Inspect uploaded CSVs (if any)
# -------------------------
file_summaries = {}
for key, p in FOUND.items():
    try:
        df = pd.read_csv(p)
        file_summaries[key] = {
            "path": str(p),
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "numeric_cols": df.select_dtypes(include=[np.number]).columns.tolist()
        }
    except Exception as e:
        file_summaries[key] = {"error": str(e)}

print("\nInput summary:")
for k, v in file_summaries.items():
    print("-", k, ":", v.get("shape", v.get("error")))

# Determine number of samples to produce / use
sample_count = 200  # [EDIT HERE IF NEEDED]
if "test_tcga" in file_summaries and "shape" in file_summaries["test_tcga"]:
    sample_count = max(sample_count, file_summaries["test_tcga"]["shape"][0])
print("Using sample_count =", sample_count)

# -------------------------
# 2) Generate Slide Images (.tif) and Extract Patches
# -------------------------
# Create a slide directory
SLIDE_DIR = OUT_DIR / "slides"
SLIDE_DIR.mkdir(exist_ok=True)

def create_slide_image(slide_id, width=1536, height=1536, texture_strength=0.7):
    """
    Create a tumor-like RGB slide image with tissue-like texture, staining hues, and variational noise.
    Saved as a tiled TIFF-like RGB PNG (PIL supports .tiff as well).
    """
    # base tissue-like background
    base = np.random.normal(loc=180, scale=30, size=(height, width, 3)).astype(np.uint8)
    # introduce H&E-like staining tint
    tint = np.zeros((height, width, 3), dtype=np.uint8)
    tint[..., 0] = np.clip(base[..., 0].astype(np.int16) - int(np.random.randint(0, 40)), 0, 255).astype(np.uint8)  # pinkish
    tint[..., 1] = np.clip(base[..., 1].astype(np.int16) - int(np.random.randint(10, 60)), 0, 255).astype(np.uint8)  # purple
    tint[..., 2] = np.clip(base[..., 2].astype(np.int16) + int(np.random.randint(-10, 10)), 0, 255).astype(np.uint8)
    # texture: Perlin-like noise approximated by Gaussian blur of random spots
    canvas = Image.fromarray(tint)
    draw = ImageDraw.Draw(canvas)
    # add random nuclei-like dark spots
    for _ in range(int((width*height) * 0.00003 * texture_strength)):
        x = random.randint(0, width-1)
        y = random.randint(0, height-1)
        r = random.randint(2, 8)
        col = (random.randint(50,100), random.randint(30,60), random.randint(80,120))
        draw.ellipse((x-r, y-r, x+r, y+r), fill=col)
    canvas = canvas.filter(ImageFilter.GaussianBlur(radius=1.2))
    # add mild vignette
    vignette = Image.new("L", (width, height), 0)
    vd = ImageDraw.Draw(vignette)
    vd.ellipse((-width*0.1, -height*0.1, width*1.1, height*1.1), fill=255)
    canvas.putalpha(vignette)
    # Save as TIFF
    slide_path = SLIDE_DIR / f"{slide_id}.tif"
    canvas.convert("RGB").save(slide_path, format="TIFF", compression="tiff_deflate")
    return slide_path

# Decide slide IDs: if test_tcga provided, use its sample IDs; else synthetic IDs
if "test_tcga" in FOUND:
    df_test = pd.read_csv(FOUND["test_tcga"])
    if "case_id" in df_test.columns:
        slide_ids = df_test["case_id"].astype(str).unique().tolist()[:sample_count]
    else:
        slide_ids = [f"SLIDE_{i:05d}" for i in range(sample_count)]
else:
    slide_ids = [f"SLIDE_{i:05d}" for i in range(sample_count)]

print("\nGenerating slide images (small tiles) â€” this may take a few seconds...")
generated_slides = []
for sid in tqdm(slide_ids[:min(len(slide_ids), 12)], desc="Slides"):  # generate up to 12 slides for demo speed
    path = create_slide_image(sid, width=1024, height=1024, texture_strength=0.85)  # [EDIT HERE IF NEEDED]
    generated_slides.append(str(path))
print("Generated slides:", len(generated_slides), "->", generated_slides[:3])

# Patch extraction function (non-overlapping)
def extract_patches(slide_path, patch_size=256, max_patches=36):
    img = Image.open(slide_path).convert("RGB")
    w, h = img.size
    patches = []
    coords = []
    # grid extraction
    nx = w // patch_size
    ny = h // patch_size
    for yi in range(ny):
        for xi in range(nx):
            if len(patches) >= max_patches:
                break
            left = xi * patch_size
            upper = yi * patch_size
            patch = img.crop((left, upper, left + patch_size, upper + patch_size))
            patches.append(patch)
            coords.append((left, upper))
        if len(patches) >= max_patches:
            break
    return patches, coords

# Extract patches for the first few slides and save sample patch thumbnails
PATCH_DIR = OUT_DIR / "patches"
PATCH_DIR.mkdir(exist_ok=True)
sample_patch_records = []
for slide in tqdm(generated_slides[:6], desc="Patches"):
    ps, coords = extract_patches(slide, patch_size=256, max_patches=36)
    for i, p in enumerate(ps[:9]):  # save 9 thumbnails per slide for report
        ppath = PATCH_DIR / f"{Path(slide).stem}_patch_{i}.png"
        p.resize((256,256)).save(ppath)
        sample_patch_records.append({"slide_id": Path(slide).stem, "patch_file": str(ppath)})
print("Saved sample patches:", len(sample_patch_records))

# -------------------------
# 3) Feature Extraction & Tumor Origin Modeling
# -------------------------
# Build a slide-level features table by summarizing patch/color statistics
def summarize_slide(slide_path, num_patches=24):
    patches, _ = extract_patches(slide_path, patch_size=256, max_patches=num_patches)
    arrs = [np.asarray(p).astype(np.float32) for p in patches]
    # per-patch summaries
    means = [a.mean(axis=(0,1)) for a in arrs]
    stds  = [a.std(axis=(0,1)) for a in arrs]
    means = np.array(means)
    stds  = np.array(stds)
    feat = {
        "slide_id": Path(slide_path).stem,
        "rgb_mean_r": float(means[:,0].mean()),
        "rgb_mean_g": float(means[:,1].mean()),
        "rgb_mean_b": float(means[:,2].mean()),
        "rgb_std_r": float(stds[:,0].mean()),
        "patch_count": len(arrs)
    }
    return feat

slide_features = []
for sp in generated_slides:
    slide_features.append(summarize_slide(sp, num_patches=24))
slide_features_df = pd.DataFrame(slide_features)
if slide_features_df.empty:
    # fallback single synthetic row
    slide_features_df = pd.DataFrame([{"slide_id":"SLIDE_00000","rgb_mean_r":150,"rgb_mean_g":130,"rgb_mean_b":140,"rgb_std_r":25,"patch_count":24}])

# Determine tumor origin labels/classes
if "test_tcga" in FOUND:
    df_test = pd.read_csv(FOUND["test_tcga"])
    label_col = "site" if "site" in df_test.columns else ( "label" if "label" in df_test.columns else None )
    if label_col:
        class_candidates = df_test[label_col].dropna().unique().tolist()
    else:
        class_candidates = ["Lung","Colon","Breast","Brain","Skin"]
else:
    class_candidates = ["Lung","Colon","Breast","Brain","Skin"]

# Train a light classifier to produce probabilistic predictions (for reporting)
# Note: model is a demonstration stub; in production you'd replace with the trained origin model.
n_samples = len(slide_features_df)
# create pseudo-targets to train on so outputs have structure
pseudo_targets = np.random.choice(class_candidates, size=n_samples)
rf_origin = RandomForestClassifier(n_estimators=120, random_state=RANDOM_SEED)
train_X = slide_features_df[["rgb_mean_r","rgb_std_r","patch_count"]].values
rf_origin.fit(train_X, pseudo_targets)
pred_proba = rf_origin.predict_proba(train_X)
pred_labels = rf_origin.classes_[np.argmax(pred_proba, axis=1)]

origin_results = slide_features_df[["slide_id"]].copy()
origin_results["predicted_origin"] = pred_labels
# store top-3 probabilities per slide in JSON-like string
def topk_probs_row(row_probs, classes, k=3):
    idx = np.argsort(row_probs)[::-1][:k]
    return json.dumps({classes[i]: float(row_probs[i]) for i in idx})
origin_results["top3_proba"] = [topk_probs_row(pred_proba[i], rf_origin.classes_.tolist(), 3) for i in range(len(pred_proba))]
save_csv(origin_results, "tumor_origin_predictions.csv")

# Plot: class distribution and feature importance
plt.figure(figsize=(8,4))
origin_results["predicted_origin"].value_counts().plot(kind="bar", color=sns.color_palette("Set2"))
plt.title("Predicted Tumor Origin Distribution")
plt.ylabel("Sample Count")
plt.tight_layout()
plt.savefig(OUT_DIR / "figures" / "origin_class_counts.png")
plt.close()

# Feature importance (RF)
fi = pd.DataFrame({
    "feature": ["rgb_mean_r","rgb_std_r","patch_count"],
    "importance": rf_origin.feature_importances_
}).sort_values("importance", ascending=True)
plt.figure(figsize=(6,3))
plt.barh(fi["feature"], fi["importance"], color=sns.color_palette("Blues", n_colors=3))
plt.title("Origin Model â€” Feature Importance")
plt.xlabel("Importance")
plt.tight_layout()
plt.savefig(OUT_DIR / "figures" / "origin_feature_importance.png")
plt.close()

# Confusion matrix (pseudo)
cm = confusion_matrix(pseudo_targets, rf_origin.predict(train_X), labels=rf_origin.classes_)
fig, ax = plt.subplots(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=rf_origin.classes_, yticklabels=rf_origin.classes_, ax=ax)
ax.set_xlabel("Predicted")
ax.set_ylabel("Reference")
ax.set_title("Confusion Matrix â€” Origin Model (demo)")
plt.tight_layout()
fig.savefig(OUT_DIR / "figures" / "origin_confusion_matrix.png")
plt.close()

# -------------------------
# 4) Cell Detection Emulation (tile-based detections)
# -------------------------
# Instead of running a heavy detector, emulate cell detections derived from patch statistics.
tiles = []
for slide_row in tqdm(list(slide_features_df.itertuples(index=False)), desc="Tiles"):
    num_tiles = int(max(8, slide_row.patch_count))
    for t in range(num_tiles):
        nid = f"{slide_row.slide_id}_T{t:03d}"
        mean_int = float(np.clip(np.random.normal(loc=slide_row.rgb_mean_r, scale=20), 10, 245))
        nuclei_score = float(np.clip(np.random.beta(2.8, 3.8) + (mean_int/255.0)*0.5, 0, 1))
        tiles.append({
            "tile_id": nid,
            "slide_id": slide_row.slide_id,
            "mean_intensity": mean_int,
            "nuclei_score": nuclei_score
        })
tiles_df = pd.DataFrame(tiles)

# Generate detection boxes and scores per tile
detections = []
for r in tqdm(list(tiles_df.itertuples(index=False)), desc="Detections"):
    count = int((r.nuclei_score > 0.55) * (1 + int(r.mean_intensity // 90)))
    for i in range(count):
        w = random.randint(12, 64)
        h = random.randint(12, 64)
        detections.append({
            "detection_id": f"{r.tile_id}_D{i}",
            "slide_id": r.slide_id,
            "tile_id": r.tile_id,
            "x": random.randint(0, 512 - w),
            "y": random.randint(0, 512 - h),
            "width": w,
            "height": h,
            "area": w*h,
            "score": round(random.uniform(0.58, 0.995), 3)
        })
detections_df = pd.DataFrame(detections)
if detections_df.empty:
    detections_df = pd.DataFrame([{
        "detection_id": "DUMMY_000",
        "slide_id": slide_features_df["slide_id"].iloc[0],
        "tile_id": tiles_df["tile_id"].iloc[0] if not tiles_df.empty else "TILE_000",
        "x": 10, "y": 10, "width": 20, "height": 20, "area": 400, "score": 0.98
    }])

save_csv(detections_df, "cell_detections.csv")

# Visual: detection count per slide (top 12)
top_det = detections_df["slide_id"].value_counts().head(12)
plt.figure(figsize=(10,4))
sns.barplot(x=top_det.index, y=top_det.values, palette="crest")
plt.xticks(rotation=45, ha="right")
plt.title("Top Slides by Cell Detections")
plt.ylabel("Detections")
plt.xlabel("Slide ID")
plt.tight_layout()
plt.savefig(OUT_DIR / "figures" / "detections_per_slide_top12.png")
plt.close()

# Simulated attention heatmap per slide (CHIEF-like visualization)
ATTN_DIR = OUT_DIR / "figures" / "attention"
ATTN_DIR.mkdir(exist_ok=True)
for sid, sdf in tiles_df.groupby("slide_id"):
    scores = sdf.sort_values("tile_id")[["nuclei_score"]].values.flatten()
    n_tiles = len(scores)
    if n_tiles < 4:
        continue
    grid_size = int(np.ceil(np.sqrt(n_tiles)))
    heat = np.full((grid_size, grid_size), np.nan, dtype=float)
    for idx, val in enumerate(scores):
        r = idx // grid_size
        c = idx % grid_size
        heat[r, c] = val
    plt.figure(figsize=(4.2, 4))
    sns.heatmap(heat, vmin=0, vmax=1, cmap="mako", cbar=True, square=True, linewidths=0.3, linecolor="white")
    plt.title(f"Attention Heatmap â€” {sid}")
    plt.xticks([]); plt.yticks([])
    plt.tight_layout()
    plt.savefig(ATTN_DIR / f"attn_{sid}.png")
    plt.close()

# -------------------------
# 5) Biomarker Aggregation & Risk Modeling
# -------------------------
# Aggregate per-slide biomarkers from tiles and detections
tile_agg = tiles_df.groupby("slide_id").agg(
    tiles_mean_intensity=("mean_intensity","mean"),
    tiles_nuclei_frac=("nuclei_score", lambda x: (x>0.55).mean()),
    tiles_count=("tile_id","count")
).reset_index()

det_agg = detections_df.groupby("slide_id").agg(
    n_detections=("detection_id","count"),
    mean_detection_score=("score","mean"),
    mean_area=("area","mean")
).reset_index()

biomarkers_df = tile_agg.merge(det_agg, on="slide_id", how="left").fillna(0)
biomarkers_df["detection_density"] = biomarkers_df["n_detections"] / biomarkers_df["tiles_count"].replace(0,1)
biomarkers_df["nuclearity_index"] = biomarkers_df["tiles_nuclei_frac"] * biomarkers_df["tiles_mean_intensity"]

# Produce a reproducible binary risk label using a quantile mix
mix_threshold = biomarkers_df["detection_density"].quantile(0.6) * 0.8 + biomarkers_df["nuclearity_index"].quantile(0.6) * 0.2
biomarkers_df["risk_label"] = (biomarkers_df["detection_density"] + 0.02*biomarkers_df["nuclearity_index"] > mix_threshold).astype(int)

# Train a compact risk model for probability outputs (demo)
risk_features = ["detection_density","nuclearity_index","mean_detection_score"]
X = biomarkers_df[risk_features].fillna(0)
y = biomarkers_df["risk_label"]
# --- COLAB PIPELINE RISK ---
if len(y.unique()) >= 2:
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=RANDOM_SEED)
    risk_model = LogisticRegression(max_iter=400)
    risk_model.fit(Xtr, ytr)
    biomarkers_df["risk_score"] = risk_model.predict_proba(X)[:,1]
    try:
        auc_demo = roc_auc_score(yte, risk_model.predict_proba(Xte)[:,1])
    except Exception:
        auc_demo = None
else:
    # Not enough classes for modeling
    biomarkers_df["risk_score"] = np.random.rand(len(biomarkers_df))
    auc_demo = None

# Risk category
biomarkers_df["risk_category"] = pd.cut(
    biomarkers_df["risk_score"], bins=[-0.01, 0.33, 0.66, 1.01], labels=["low","medium","high"]
).astype(str)

save_csv(biomarkers_df, "biomarker_panel.csv")
save_csv(biomarkers_df[["slide_id","risk_score","risk_category"]], "risk_scores.csv")

# Figure: correlation heatmap of biomarkers/features
fig, ax = plt.subplots(figsize=(6,5))
corr = biomarkers_df[["tiles_mean_intensity","tiles_nuclei_frac","n_detections","detection_density","nuclearity_index","risk_score"]].corr()
sns.heatmap(corr, annot=True, fmt=".2f", cmap="vlag", center=0, ax=ax)
ax.set_title("Biomarker Feature Correlation")
plt.tight_layout()
fig.savefig(OUT_DIR / "figures" / "biomarker_correlation_heatmap.png")
plt.close()

# Violin: risk score distribution
plt.figure(figsize=(6,4))
sns.violinplot(y=biomarkers_df["risk_score"], inner="quartile", color="lightsteelblue")
plt.title("Patient Risk Score Distribution")
plt.ylabel("Risk Score (probability)")
plt.tight_layout()
plt.savefig(OUT_DIR / "figures" / "risk_score_violin.png")
plt.close()

# Calibration curve (binned reliability)
try:
    eval_df = biomarkers_df[["risk_score", "risk_label"]].dropna().copy()
    eval_df["bin"] = pd.qcut(eval_df["risk_score"], q=min(10, max(3, len(eval_df)//3)), duplicates="drop")
    calib = eval_df.groupby("bin").agg(
        pred_mean=("risk_score", "mean"),
        obs_rate=("risk_label", "mean"),
        count=("risk_label", "size")
    ).reset_index(drop=True)
    plt.figure(figsize=(5.6, 5.0))
    sns.lineplot(x=calib["pred_mean"], y=calib["obs_rate"], marker="o", linewidth=2, color=PALETTE[3])
    plt.plot([0,1],[0,1], linestyle="--", color="gray", linewidth=1)
    plt.xlabel("Predicted risk")
    plt.ylabel("Observed frequency")
    plt.title("Risk Calibration (binned)")
    plt.tight_layout()
    plt.savefig(OUT_DIR / "figures" / "risk_calibration.png")
    plt.close()
except Exception:
    pass

# ROC-like curve (threshold sweep)
try:
    y_true = y.values.astype(int)
    y_score = biomarkers_df["risk_score"].values
    thresholds = np.linspace(0, 1, 101)
    tpr_list, fpr_list = [], []
    for th in thresholds:
        y_pred = (y_score >= th).astype(int)
        tp = np.sum((y_true==1) & (y_pred==1))
        fp = np.sum((y_true==0) & (y_pred==1))
        fn = np.sum((y_true==1) & (y_pred==0))
        tn = np.sum((y_true==0) & (y_pred==0))
        tpr = tp / (tp + fn + 1e-9)
        fpr = fp / (fp + tn + 1e-9)
        tpr_list.append(tpr)
        fpr_list.append(fpr)
    plt.figure(figsize=(5.6, 5.0))
    sns.lineplot(x=fpr_list, y=tpr_list, color=PALETTE[2], linewidth=2)
    plt.plot([0,1],[0,1], linestyle="--", color="gray", linewidth=1)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    title_auc = f"ROC-like Curve (AUCâ‰ˆ{auc_demo:.3f})" if 'auc_demo' in locals() and auc_demo is not None else "ROC-like Curve"
    plt.title(title_auc)
    plt.tight_layout()
    plt.savefig(OUT_DIR / "figures" / "risk_roc_like.png")
    plt.close()
except Exception:
    pass

# Simple survival-like curves for low/medium/high groups (synthetic but polished)
km_df = biomarkers_df.copy()
# create synthetic 'time' and 'event' correlated with risk_score
km_df["time_months"] = (np.random.exponential(scale=30, size=len(km_df)) * (1 + km_df["risk_score"])).round(1)
km_df["event_observed"] = (km_df["risk_score"] > 0.6).astype(int)

plt.figure(figsize=(6,4))
for grp_label in ["low","medium","high"]:
    grp = km_df[km_df["risk_category"]==grp_label]
    if grp.empty:
        continue
    # create a step-like mean survival curve for display
    times = np.sort(grp["time_months"].values)
    if len(times) < 2:
        continue
    survival = 1 - np.linspace(0, 0.9, len(times)) * (0.6 + 0.4*({"low":0.2,"medium":0.6,"high":1.0}[grp_label]))
    plt.step(times, survival, where="post", label=f"{grp_label.capitalize()} (n={len(grp)})")
plt.xlabel("Months")
plt.ylabel("Survival Probability (est.)")
plt.title("Survival-Like Curves by Risk Group (Illustrative)")
plt.legend()
plt.tight_layout()
plt.savefig(OUT_DIR / "figures" / "survival_like_by_risk.png")
plt.close()

# -------------------------
# 6) Compose Final Figure Panel & Methods JSON
# -------------------------
panel_files = [
    "origin_class_counts.png",
    "origin_feature_importance.png",
    "origin_confusion_matrix.png",
    "detections_per_slide_top12.png",
    "biomarker_correlation_heatmap.png",
    "risk_score_violin.png",
    "survival_like_by_risk.png"
]
# copy existing generated figures (some may not exist)
existing_panels = [OUT_DIR / "figures" / f for f in panel_files if (OUT_DIR / "figures" / f).exists()]

# Create a horizontal panel (stitch) if we have at least 3 figures
if len(existing_panels) >= 3:
    imgs = [Image.open(p).resize((600, 360)) for p in existing_panels[:3]]
    total_w = sum(im.width for im in imgs)
    max_h = max(im.height for im in imgs)
    canvas = Image.new("RGB", (total_w, max_h), (255,255,255))
    x = 0
    for im in imgs:
        canvas.paste(im, (x,0))
        x += im.width
    canvas.save(OUT_DIR / "figures" / "figure_panel_overview.png")

methods = {
    "pipeline_name": "Histology Image Analysis â€” Reporting Pipeline",
    "slides_used": len(generated_slides),
    "samples_processed": len(biomarkers_df),
    "models": {
        "origin_model": "RandomForestClassifier (demo)",
        "risk_model": "LogisticRegression (demo)"
    },
    "outputs": {
        "tumor_origin_predictions": str(OUT_DIR / "tumor_origin_predictions.csv"),
        "cell_detections": str(OUT_DIR / "cell_detections.csv"),
        "biomarker_panel": str(OUT_DIR / "biomarker_panel.csv"),
        "risk_scores": str(OUT_DIR / "risk_scores.csv"),
        "figures": str(OUT_DIR / "figures")
    }
}
with open(OUT_DIR / "methods_overview.json", "w") as f:
    json.dump(methods, f, indent=2)

# -------------------------
# 7) Save sample summary and print outputs
# -------------------------
sample_summary = biomarkers_df[["slide_id","tiles_count","n_detections","detection_density","tiles_nuclei_frac","risk_score","risk_category"]]
save_csv(sample_summary, "sample_summary.csv")

print("\nPipeline completed. Outputs written to:", OUT_DIR)
print("Figures (examples):")
for f in sorted((OUT_DIR / "figures").glob("*.png")):
    print("-", f.name)
print("\nCSV outputs:")
for f in sorted(OUT_DIR.glob("*.csv")):
    print("-", f.name)
print("\nJSON outputs:")
for f in sorted(OUT_DIR.glob("*.json")):
    print("-", f.name)

print("\nMethods (for camera/demo):")
print("- Inspired by CHIEF: Clinical Histopathology Imaging Evaluation Foundation Model [see repo]")
print("- This notebook simulates CHIEF-style outputs: tumor origin, cell detections, biomarkers, risk.")
print("- Attention heatmaps approximate slide-level importance by nuclei-rich tiles.")
print("- Risk calibration and ROC-like curves show model behavior on synthetic labels.")
print("- Replace demo models with CHIEF encoders and downstream heads for real inference.")
print("Repo:", "https://github.com/hms-dbmi/CHIEF")

